#!/usr/bin/env python3
import os

'''
persistent file database for url crawled or wait for crawling.
        url sha1, url, ref sha1, ref
'''
class urldb:
    def __init__(self, dbpath):
        self.open(dbpath)
        
    def open(self, dbpath):
        if(not os.path.exists(dbpath)):
            os.makedirs(dbpath)

        self._urlsdbf = dbpath+"/urls.db"
        if(not os.path.isfile(self._urlsdbf)):
            f = open(self._urlsdbf, 'w', encoding="utf-8")
            f.close()
            
        self._freshdbf = dbpath+"/fresh.db"
        if(not os.path.isfile(self._freshdbf)):
            f = open(self._freshdbf, 'w', encoding="utf-8")
            f.close()

        self._urlsdb = open(self._urlsdbf, 'r+', encoding="utf-8")
        self._allurls = {}
        for line in self._urlsdb:
            line.strip('\n')
            url, ref = line.split(',')
            ref.strip()
            self._allurls[url] = ref

        self._freshdb = open(self._freshdbf, 'r+', encoding="utf-8")
        self._newurls = []
        for line in self._freshdb:
            line.strip('\n')
            url, ref = line.split(',')
            ref.strip()
            self._newurls.append([url, ref])
        self._freshdb.close()
        
    def put(self, newurls):
        for url, ref in newurls:
            if(url not in self._allurls):
                self._allurls[url] = ref
                self._newurls.append([url, ref])
                self._urlsdb.write(url+","+ref+"\n")
                
    def get(self, number):
        urls = self._newurls[0:number]
        del self._newurls[0:number]
        return urls
    
    def sync(self):
        self._freshdb = open(self._freshdbf, 'w', encoding="utf-8")
        for url, ref in self._newurls:
            self._freshdb.write(url+","+ref+"\n")
        self._freshdb.flush()
        self._freshdb.close()
        
        self._urlsdb.flush()
        
    def close(self):
        self._urlsdb.close()
        self._freshdb.close()
    

if(__name__ == "__main__"):
    db = urldb("../../db")
    newurls = []
    for i in range(0, 100000):
        newurls.append(["http://wwww.baidu.com/"+str(i), "http://wwww.sohu.com/"+str(i)])
    db.put(newurls)
    print(db.get(10))
    db.sync()